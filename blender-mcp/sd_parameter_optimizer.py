#!/usr/bin/env python3
"""
Stable Diffusion å‚æ•°ä¼˜åŒ–å™¨

è¿™ä¸ªæ¨¡å—æä¾›æ™ºèƒ½å‚æ•°ä¼˜åŒ–åŠŸèƒ½ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚ã€ç¡¬ä»¶é…ç½®å’Œç”Ÿæˆç›®æ ‡
è‡ªåŠ¨æ¨èæœ€ä½³çš„ Stable Diffusion å‚æ•°ç»„åˆã€‚

ä¸»è¦åŠŸèƒ½:
- æ™ºèƒ½å‚æ•°æ¨è
- ç¡¬ä»¶æ€§èƒ½ä¼˜åŒ–
- è´¨é‡ä¸é€Ÿåº¦å¹³è¡¡
- æ‰¹é‡ç”Ÿæˆä¼˜åŒ–
- å®æ—¶å‚æ•°è°ƒæ•´å»ºè®®
"""

import json
import math
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from sd_optimization_presets import get_preset, PRESETS, SDPreset

class OptimizationGoal(Enum):
    """ä¼˜åŒ–ç›®æ ‡æšä¸¾"""
    SPEED = "speed"          # ä¼˜å…ˆé€Ÿåº¦
    QUALITY = "quality"      # ä¼˜å…ˆè´¨é‡
    BALANCED = "balanced"    # å¹³è¡¡æ¨¡å¼
    MEMORY = "memory"        # å†…å­˜ä¼˜åŒ–
    BATCH = "batch"          # æ‰¹é‡ä¼˜åŒ–
    CREATIVE = "creative"    # åˆ›æ„æ¢ç´¢

class HardwareProfile(Enum):
    """ç¡¬ä»¶é…ç½®æšä¸¾"""
    LOW_END = "low_end"      # ä½ç«¯é…ç½® (<4GB VRAM)
    MID_RANGE = "mid_range"  # ä¸­ç«¯é…ç½® (4-8GB VRAM)
    HIGH_END = "high_end"    # é«˜ç«¯é…ç½® (8-12GB VRAM)
    ENTHUSIAST = "enthusiast" # å‘çƒ§çº§é…ç½® (>12GB VRAM)

@dataclass
class OptimizationContext:
    """ä¼˜åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    goal: OptimizationGoal
    hardware_profile: HardwareProfile
    gpu_memory_gb: float
    target_resolution: Tuple[int, int] = (512, 512)
    time_constraint: Optional[int] = None  # ç§’
    quality_threshold: float = 0.7  # 0-1
    batch_requirement: int = 1
    use_case: str = "general"
    model_type: str = "sd15"  # sd15, sdxl, etc.
    
@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    recommended_params: Dict[str, Any]
    confidence_score: float
    estimated_time: float
    memory_usage: float
    quality_score: float
    alternative_configs: List[Dict[str, Any]]
    optimization_notes: List[str]
    warnings: List[str]

class SDParameterOptimizer:
    """Stable Diffusion å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.performance_cache = {}
        self.optimization_history = []
        
        # ç¡¬ä»¶é…ç½®æ˜ å°„
        self.hardware_configs = {
            HardwareProfile.LOW_END: {
                "max_resolution": (512, 512),
                "max_batch_size": 1,
                "recommended_steps": (8, 20),
                "enable_hr": False,
                "memory_limit": 4.0
            },
            HardwareProfile.MID_RANGE: {
                "max_resolution": (768, 768),
                "max_batch_size": 2,
                "recommended_steps": (15, 30),
                "enable_hr": True,
                "memory_limit": 8.0
            },
            HardwareProfile.HIGH_END: {
                "max_resolution": (1024, 1024),
                "max_batch_size": 4,
                "recommended_steps": (20, 50),
                "enable_hr": True,
                "memory_limit": 12.0
            },
            HardwareProfile.ENTHUSIAST: {
                "max_resolution": (1536, 1536),
                "max_batch_size": 8,
                "recommended_steps": (25, 80),
                "enable_hr": True,
                "memory_limit": 24.0
            }
        }
        
        # é‡‡æ ·å™¨æ€§èƒ½ç‰¹å¾
        self.sampler_profiles = {
            "DPM++ 2M Karras": {"speed": 0.9, "quality": 0.8, "stability": 0.9},
            "DPM++ SDE Karras": {"speed": 0.6, "quality": 0.95, "stability": 0.8},
            "Euler a": {"speed": 0.95, "quality": 0.7, "stability": 0.6},
            "Euler": {"speed": 0.9, "quality": 0.75, "stability": 0.8},
            "DDIM": {"speed": 0.7, "quality": 0.85, "stability": 0.95},
            "PLMS": {"speed": 0.8, "quality": 0.8, "stability": 0.85},
            "LMS Karras": {"speed": 0.85, "quality": 0.75, "stability": 0.8},
            "DPM++ 2S a Karras": {"speed": 0.5, "quality": 0.9, "stability": 0.85},
            "Heun": {"speed": 0.4, "quality": 0.85, "stability": 0.9}
        }
    
    def optimize_parameters(self, context: OptimizationContext) -> OptimizationResult:
        """
        æ ¹æ®ä¼˜åŒ–ä¸Šä¸‹æ–‡æ¨èæœ€ä½³å‚æ•°
        
        Args:
            context: ä¼˜åŒ–ä¸Šä¸‹æ–‡
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        # è·å–ç¡¬ä»¶é™åˆ¶
        hw_config = self.hardware_configs[context.hardware_profile]
        
        # é€‰æ‹©åŸºç¡€é¢„è®¾
        base_preset = self._select_base_preset(context)
        
        # åº”ç”¨ä¼˜åŒ–ç­–ç•¥
        optimized_params = self._apply_optimization_strategy(base_preset, context, hw_config)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_metrics = self._calculate_performance_metrics(optimized_params, context)
        
        # ç”Ÿæˆæ›¿ä»£é…ç½®
        alternatives = self._generate_alternatives(optimized_params, context, hw_config)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®å’Œè­¦å‘Š
        notes, warnings = self._generate_recommendations(optimized_params, context, performance_metrics)
        
        result = OptimizationResult(
            recommended_params=optimized_params,
            confidence_score=performance_metrics["confidence"],
            estimated_time=performance_metrics["estimated_time"],
            memory_usage=performance_metrics["memory_usage"],
            quality_score=performance_metrics["quality_score"],
            alternative_configs=alternatives,
            optimization_notes=notes,
            warnings=warnings
        )
        
        # è®°å½•ä¼˜åŒ–å†å²
        self.optimization_history.append({
            "timestamp": time.time(),
            "context": asdict(context),
            "result": asdict(result)
        })
        
        return result
    
    def _select_base_preset(self, context: OptimizationContext) -> Dict[str, Any]:
        """
        é€‰æ‹©æœ€é€‚åˆçš„åŸºç¡€é¢„è®¾
        """
        goal_preset_mapping = {
            OptimizationGoal.SPEED: "quick_preview",
            OptimizationGoal.QUALITY: "high_quality",
            OptimizationGoal.BALANCED: "standard",
            OptimizationGoal.MEMORY: "quick_preview",
            OptimizationGoal.BATCH: "batch_generation",
            OptimizationGoal.CREATIVE: "creative_exploration"
        }
        
        # æ ¹æ®ç”¨ä¾‹è°ƒæ•´
        if context.use_case == "portrait":
            preset_name = "portrait"
        elif context.use_case == "landscape":
            preset_name = "landscape"
        elif context.use_case == "3d_modeling":
            preset_name = "for_3d_model"
        else:
            preset_name = goal_preset_mapping.get(context.goal, "standard")
        
        return get_preset(preset_name) or get_preset("standard")
    
    def _apply_optimization_strategy(self, base_params: Dict[str, Any], 
                                   context: OptimizationContext, 
                                   hw_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åº”ç”¨å…·ä½“çš„ä¼˜åŒ–ç­–ç•¥
        """
        params = base_params.copy()
        
        # ç¡¬ä»¶é™åˆ¶ä¼˜åŒ–
        params = self._apply_hardware_constraints(params, context, hw_config)
        
        # ç›®æ ‡å¯¼å‘ä¼˜åŒ–
        if context.goal == OptimizationGoal.SPEED:
            params = self._optimize_for_speed(params, context)
        elif context.goal == OptimizationGoal.QUALITY:
            params = self._optimize_for_quality(params, context)
        elif context.goal == OptimizationGoal.MEMORY:
            params = self._optimize_for_memory(params, context)
        elif context.goal == OptimizationGoal.BATCH:
            params = self._optimize_for_batch(params, context)
        elif context.goal == OptimizationGoal.CREATIVE:
            params = self._optimize_for_creativity(params, context)
        else:  # BALANCED
            params = self._optimize_balanced(params, context)
        
        # æ—¶é—´çº¦æŸä¼˜åŒ–
        if context.time_constraint:
            params = self._apply_time_constraints(params, context)
        
        return params
    
    def _apply_hardware_constraints(self, params: Dict[str, Any], 
                                  context: OptimizationContext, 
                                  hw_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åº”ç”¨ç¡¬ä»¶çº¦æŸ
        """
        # åˆ†è¾¨ç‡é™åˆ¶
        max_w, max_h = hw_config["max_resolution"]
        target_w, target_h = context.target_resolution
        
        if target_w > max_w or target_h > max_h:
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾åˆ°ç¡¬ä»¶é™åˆ¶å†…
            scale = min(max_w / target_w, max_h / target_h)
            params["width"] = int(target_w * scale // 64) * 64  # ç¡®ä¿æ˜¯64çš„å€æ•°
            params["height"] = int(target_h * scale // 64) * 64
        else:
            params["width"] = target_w
            params["height"] = target_h
        
        # æ‰¹æ¬¡å¤§å°é™åˆ¶
        params["batch_size"] = min(params.get("batch_size", 1), hw_config["max_batch_size"])
        
        # é«˜åˆ†è¾¨ç‡ä¿®å¤é™åˆ¶
        if not hw_config["enable_hr"]:
            params["enable_hr"] = False
        
        # æ­¥æ•°é™åˆ¶
        min_steps, max_steps = hw_config["recommended_steps"]
        params["steps"] = max(min_steps, min(params.get("steps", 20), max_steps))
        
        return params
    
    def _optimize_for_speed(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        é€Ÿåº¦ä¼˜åŒ–
        """
        # é€‰æ‹©æœ€å¿«çš„é‡‡æ ·å™¨
        fast_samplers = ["DPM++ 2M Karras", "Euler a", "LMS Karras"]
        params["sampler_name"] = fast_samplers[0]
        
        # å‡å°‘æ­¥æ•°
        params["steps"] = min(params.get("steps", 20), 15)
        
        # ç¦ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤
        params["enable_hr"] = False
        
        # ç¦ç”¨é¢éƒ¨ä¿®å¤
        params["restore_faces"] = False
        
        # é™ä½CFG Scale
        params["cfg_scale"] = min(params.get("cfg_scale", 7.0), 6.0)
        
        return params
    
    def _optimize_for_quality(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        è´¨é‡ä¼˜åŒ–
        """
        # é€‰æ‹©é«˜è´¨é‡é‡‡æ ·å™¨
        quality_samplers = ["DPM++ SDE Karras", "DPM++ 2S a Karras", "DDIM"]
        params["sampler_name"] = quality_samplers[0]
        
        # å¢åŠ æ­¥æ•°
        params["steps"] = max(params.get("steps", 20), 25)
        
        # å¯ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤
        if context.hardware_profile != HardwareProfile.LOW_END:
            params["enable_hr"] = True
            params["hr_scale"] = 1.5
        
        # å¯ç”¨é¢éƒ¨ä¿®å¤
        params["restore_faces"] = True
        
        # ä¼˜åŒ–CFG Scale
        params["cfg_scale"] = max(params.get("cfg_scale", 7.0), 8.0)
        
        return params
    
    def _optimize_for_memory(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        å†…å­˜ä¼˜åŒ–
        """
        # æœ€å°æ‰¹æ¬¡å¤§å°
        params["batch_size"] = 1
        params["n_iter"] = 1
        
        # ç¦ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤
        params["enable_hr"] = False
        
        # é™ä½åˆ†è¾¨ç‡
        params["width"] = min(params.get("width", 512), 512)
        params["height"] = min(params.get("height", 512), 512)
        
        # é€‰æ‹©å†…å­˜å‹å¥½çš„é‡‡æ ·å™¨
        params["sampler_name"] = "DPM++ 2M Karras"
        
        return params
    
    def _optimize_for_batch(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        æ‰¹é‡ç”Ÿæˆä¼˜åŒ–
        """
        # å¹³è¡¡æ‰¹æ¬¡å¤§å°å’Œè´¨é‡
        max_batch = self.hardware_configs[context.hardware_profile]["max_batch_size"]
        params["batch_size"] = min(context.batch_requirement, max_batch)
        
        # é€‚ä¸­çš„æ­¥æ•°
        params["steps"] = 15
        
        # é€‰æ‹©ç¨³å®šçš„é‡‡æ ·å™¨
        params["sampler_name"] = "DPM++ 2M Karras"
        
        # ç¦ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤ä»¥èŠ‚çœæ—¶é—´
        params["enable_hr"] = False
        
        return params
    
    def _optimize_for_creativity(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        åˆ›æ„ä¼˜åŒ–
        """
        # é€‰æ‹©åˆ›æ„é‡‡æ ·å™¨
        creative_samplers = ["Euler a", "Euler", "Heun"]
        params["sampler_name"] = creative_samplers[0]
        
        # é™ä½CFG Scaleä»¥å¢åŠ åˆ›æ„æ€§
        params["cfg_scale"] = min(params.get("cfg_scale", 7.0), 6.0)
        
        # é€‚ä¸­çš„æ­¥æ•°
        params["steps"] = 20
        
        # å¢åŠ æ‰¹æ¬¡ä»¥è·å¾—æ›´å¤šå˜ä½“
        max_batch = self.hardware_configs[context.hardware_profile]["max_batch_size"]
        params["batch_size"] = min(2, max_batch)
        
        return params
    
    def _optimize_balanced(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        å¹³è¡¡ä¼˜åŒ–
        """
        # é€‰æ‹©å¹³è¡¡çš„é‡‡æ ·å™¨
        params["sampler_name"] = "DPM++ 2M Karras"
        
        # å¹³è¡¡çš„æ­¥æ•°
        params["steps"] = 20
        
        # å¹³è¡¡çš„CFG Scale
        params["cfg_scale"] = 7.0
        
        # æ ¹æ®ç¡¬ä»¶å†³å®šæ˜¯å¦å¯ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤
        if context.hardware_profile in [HardwareProfile.HIGH_END, HardwareProfile.ENTHUSIAST]:
            params["enable_hr"] = True
            params["hr_scale"] = 1.3
        
        return params
    
    def _apply_time_constraints(self, params: Dict[str, Any], context: OptimizationContext) -> Dict[str, Any]:
        """
        åº”ç”¨æ—¶é—´çº¦æŸ
        """
        estimated_time = self._estimate_generation_time(params, context)
        
        if estimated_time > context.time_constraint:
            # éœ€è¦åŠ é€Ÿ
            reduction_factor = context.time_constraint / estimated_time
            
            # æŒ‰ä¼˜å…ˆçº§å‡å°‘å‚æ•°
            if reduction_factor < 0.5:
                # å¤§å¹…å‡å°‘
                params["steps"] = max(8, int(params["steps"] * 0.5))
                params["enable_hr"] = False
                params["batch_size"] = 1
            elif reduction_factor < 0.8:
                # é€‚åº¦å‡å°‘
                params["steps"] = max(10, int(params["steps"] * 0.7))
                if params.get("enable_hr"):
                    params["hr_scale"] = min(params.get("hr_scale", 1.5), 1.3)
        
        return params
    
    def _calculate_performance_metrics(self, params: Dict[str, Any], 
                                     context: OptimizationContext) -> Dict[str, float]:
        """
        è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        """
        # ä¼°ç®—ç”Ÿæˆæ—¶é—´
        estimated_time = self._estimate_generation_time(params, context)
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨
        memory_usage = self._estimate_memory_usage(params, context)
        
        # ä¼°ç®—è´¨é‡åˆ†æ•°
        quality_score = self._estimate_quality_score(params, context)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(params, context, estimated_time, memory_usage, quality_score)
        
        return {
            "estimated_time": estimated_time,
            "memory_usage": memory_usage,
            "quality_score": quality_score,
            "confidence": confidence
        }
    
    def _estimate_generation_time(self, params: Dict[str, Any], context: OptimizationContext) -> float:
        """
        ä¼°ç®—ç”Ÿæˆæ—¶é—´ï¼ˆç§’ï¼‰
        """
        # åŸºç¡€æ—¶é—´è®¡ç®—
        base_time = 2.0  # åŸºç¡€æ—¶é—´
        
        # åˆ†è¾¨ç‡å½±å“
        pixels = params.get("width", 512) * params.get("height", 512)
        resolution_factor = pixels / (512 * 512)
        
        # æ­¥æ•°å½±å“
        steps_factor = params.get("steps", 20) / 20
        
        # æ‰¹æ¬¡å½±å“
        batch_factor = params.get("batch_size", 1) * params.get("n_iter", 1)
        
        # é‡‡æ ·å™¨å½±å“
        sampler_speed = self.sampler_profiles.get(params.get("sampler_name", "DPM++ 2M Karras"), {}).get("speed", 0.8)
        sampler_factor = 1.0 / sampler_speed
        
        # é«˜åˆ†è¾¨ç‡ä¿®å¤å½±å“
        hr_factor = 1.5 if params.get("enable_hr", False) else 1.0
        
        # ç¡¬ä»¶å½±å“
        hardware_multiplier = {
            HardwareProfile.LOW_END: 2.0,
            HardwareProfile.MID_RANGE: 1.0,
            HardwareProfile.HIGH_END: 0.6,
            HardwareProfile.ENTHUSIAST: 0.3
        }.get(context.hardware_profile, 1.0)
        
        total_time = (base_time * resolution_factor * steps_factor * 
                     batch_factor * sampler_factor * hr_factor * hardware_multiplier)
        
        return total_time
    
    def _estimate_memory_usage(self, params: Dict[str, Any], context: OptimizationContext) -> float:
        """
        ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆGBï¼‰
        """
        # åŸºç¡€å†…å­˜ä½¿ç”¨
        base_memory = 2.0
        
        # åˆ†è¾¨ç‡å½±å“
        pixels = params.get("width", 512) * params.get("height", 512)
        resolution_factor = pixels / (512 * 512)
        
        # æ‰¹æ¬¡å½±å“
        batch_factor = params.get("batch_size", 1)
        
        # é«˜åˆ†è¾¨ç‡ä¿®å¤å½±å“
        hr_factor = 1.3 if params.get("enable_hr", False) else 1.0
        
        total_memory = base_memory * resolution_factor * batch_factor * hr_factor
        
        return total_memory
    
    def _estimate_quality_score(self, params: Dict[str, Any], context: OptimizationContext) -> float:
        """
        ä¼°ç®—è´¨é‡åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # æ­¥æ•°å½±å“
        steps = params.get("steps", 20)
        if steps >= 25:
            score += 0.2
        elif steps >= 15:
            score += 0.1
        elif steps < 10:
            score -= 0.1
        
        # é‡‡æ ·å™¨å½±å“
        sampler_quality = self.sampler_profiles.get(params.get("sampler_name", "DPM++ 2M Karras"), {}).get("quality", 0.8)
        score += (sampler_quality - 0.8) * 0.3
        
        # CFG Scaleå½±å“
        cfg = params.get("cfg_scale", 7.0)
        if 6.0 <= cfg <= 10.0:
            score += 0.1
        elif cfg > 15.0 or cfg < 3.0:
            score -= 0.1
        
        # é«˜åˆ†è¾¨ç‡ä¿®å¤å½±å“
        if params.get("enable_hr", False):
            score += 0.15
        
        # é¢éƒ¨ä¿®å¤å½±å“
        if params.get("restore_faces", False):
            score += 0.05
        
        return max(0.0, min(1.0, score))
    
    def _calculate_confidence(self, params: Dict[str, Any], context: OptimizationContext,
                            estimated_time: float, memory_usage: float, quality_score: float) -> float:
        """
        è®¡ç®—æ¨èç½®ä¿¡åº¦
        """
        confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
        
        # ç¡¬ä»¶åŒ¹é…åº¦
        hw_limit = self.hardware_configs[context.hardware_profile]["memory_limit"]
        if memory_usage <= hw_limit * 0.8:
            confidence += 0.1
        elif memory_usage > hw_limit:
            confidence -= 0.3
        
        # æ—¶é—´çº¦æŸåŒ¹é…åº¦
        if context.time_constraint:
            if estimated_time <= context.time_constraint:
                confidence += 0.1
            else:
                confidence -= 0.2
        
        # è´¨é‡é˜ˆå€¼åŒ¹é…åº¦
        if quality_score >= context.quality_threshold:
            confidence += 0.1
        else:
            confidence -= 0.1
        
        return max(0.0, min(1.0, confidence))
    
    def _generate_alternatives(self, base_params: Dict[str, Any], 
                             context: OptimizationContext, 
                             hw_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ›¿ä»£é…ç½®
        """
        alternatives = []
        
        # é€Ÿåº¦ä¼˜åŒ–ç‰ˆæœ¬
        if context.goal != OptimizationGoal.SPEED:
            speed_params = base_params.copy()
            speed_params = self._optimize_for_speed(speed_params, context)
            speed_params["_variant_name"] = "é€Ÿåº¦ä¼˜åŒ–ç‰ˆæœ¬"
            alternatives.append(speed_params)
        
        # è´¨é‡ä¼˜åŒ–ç‰ˆæœ¬
        if context.goal != OptimizationGoal.QUALITY:
            quality_params = base_params.copy()
            quality_params = self._optimize_for_quality(quality_params, context)
            quality_params["_variant_name"] = "è´¨é‡ä¼˜åŒ–ç‰ˆæœ¬"
            alternatives.append(quality_params)
        
        # å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        if context.goal != OptimizationGoal.MEMORY:
            memory_params = base_params.copy()
            memory_params = self._optimize_for_memory(memory_params, context)
            memory_params["_variant_name"] = "å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"
            alternatives.append(memory_params)
        
        return alternatives[:3]  # æœ€å¤šè¿”å›3ä¸ªæ›¿ä»£æ–¹æ¡ˆ
    
    def _generate_recommendations(self, params: Dict[str, Any], 
                                context: OptimizationContext,
                                metrics: Dict[str, float]) -> Tuple[List[str], List[str]]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®å’Œè­¦å‘Š
        """
        notes = []
        warnings = []
        
        # æ€§èƒ½å»ºè®®
        if metrics["estimated_time"] > 60:
            notes.append("ç”Ÿæˆæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®é™ä½åˆ†è¾¨ç‡æˆ–æ­¥æ•°ä»¥æé«˜é€Ÿåº¦")
        
        if metrics["memory_usage"] > context.gpu_memory_gb * 0.9:
            warnings.append("å†…å­˜ä½¿ç”¨æ¥è¿‘ä¸Šé™ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆå¤±è´¥")
            notes.append("å»ºè®®é™ä½æ‰¹æ¬¡å¤§å°æˆ–åˆ†è¾¨ç‡")
        
        if metrics["quality_score"] < context.quality_threshold:
            notes.append("å½“å‰è®¾ç½®å¯èƒ½æ— æ³•è¾¾åˆ°æœŸæœ›çš„è´¨é‡æ°´å¹³")
            notes.append("å»ºè®®å¢åŠ æ­¥æ•°æˆ–å¯ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤")
        
        # å‚æ•°å»ºè®®
        if params.get("cfg_scale", 7.0) > 12:
            warnings.append("CFG Scaleè¿‡é«˜å¯èƒ½å¯¼è‡´è¿‡åº¦é¥±å’Œ")
        
        if params.get("steps", 20) < 10:
            warnings.append("æ­¥æ•°è¿‡å°‘å¯èƒ½å½±å“å›¾åƒè´¨é‡")
        
        # ç¡¬ä»¶å»ºè®®
        if context.hardware_profile == HardwareProfile.LOW_END:
            notes.append("å½“å‰ç¡¬ä»¶é…ç½®è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨å¿«é€Ÿé¢„è®¾")
        
        return notes, warnings
    
    def get_optimization_suggestions(self, current_params: Dict[str, Any], 
                                   performance_feedback: Dict[str, float]) -> List[str]:
        """
        æ ¹æ®æ€§èƒ½åé¦ˆæä¾›ä¼˜åŒ–å»ºè®®
        
        Args:
            current_params: å½“å‰å‚æ•°
            performance_feedback: æ€§èƒ½åé¦ˆ {"generation_time": float, "memory_used": float, "quality_rating": float}
            
        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        suggestions = []
        
        actual_time = performance_feedback.get("generation_time", 0)
        actual_memory = performance_feedback.get("memory_used", 0)
        quality_rating = performance_feedback.get("quality_rating", 0.5)
        
        # æ—¶é—´ä¼˜åŒ–å»ºè®®
        if actual_time > 120:  # è¶…è¿‡2åˆ†é’Ÿ
            suggestions.append("ç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ï¼š")
            suggestions.append("- å‡å°‘æ­¥æ•°åˆ°15-20")
            suggestions.append("- ä½¿ç”¨æ›´å¿«çš„é‡‡æ ·å™¨å¦‚ 'DPM++ 2M Karras'")
            suggestions.append("- é™ä½åˆ†è¾¨ç‡")
            if current_params.get("enable_hr"):
                suggestions.append("- ç¦ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤")
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if actual_memory > 6.0:  # è¶…è¿‡6GB
            suggestions.append("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®ï¼š")
            suggestions.append("- å‡å°‘æ‰¹æ¬¡å¤§å°")
            suggestions.append("- é™ä½åˆ†è¾¨ç‡")
            suggestions.append("- ç¦ç”¨é«˜åˆ†è¾¨ç‡ä¿®å¤")
        
        # è´¨é‡ä¼˜åŒ–å»ºè®®
        if quality_rating < 0.6:
            suggestions.append("è´¨é‡ä¸æ»¡æ„ï¼Œå»ºè®®ï¼š")
            suggestions.append("- å¢åŠ æ­¥æ•°åˆ°25-30")
            suggestions.append("- ä½¿ç”¨é«˜è´¨é‡é‡‡æ ·å™¨å¦‚ 'DPM++ SDE Karras'")
            suggestions.append("- å¯ç”¨é¢éƒ¨ä¿®å¤")
            suggestions.append("- è°ƒæ•´CFG Scaleåˆ°7-9")
        
        return suggestions
    
    def save_optimization_profile(self, name: str, context: OptimizationContext, 
                                result: OptimizationResult, filename: str = "optimization_profiles.json"):
        """
        ä¿å­˜ä¼˜åŒ–é…ç½®æ–‡ä»¶
        """
        profile = {
            "name": name,
            "context": asdict(context),
            "recommended_params": result.recommended_params,
            "performance_metrics": {
                "estimated_time": result.estimated_time,
                "memory_usage": result.memory_usage,
                "quality_score": result.quality_score,
                "confidence_score": result.confidence_score
            },
            "timestamp": time.time()
        }
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                profiles = json.load(f)
        except FileNotFoundError:
            profiles = {}
        
        profiles[name] = profile
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(profiles, f, ensure_ascii=False, indent=2)
        
        print(f"ä¼˜åŒ–é…ç½® '{name}' å·²ä¿å­˜åˆ° {filename}")
    
    def load_optimization_profile(self, name: str, filename: str = "optimization_profiles.json") -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ä¼˜åŒ–é…ç½®æ–‡ä»¶
        """
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                profiles = json.load(f)
            return profiles.get(name)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
            return None

# ä¾¿æ·å‡½æ•°
def quick_optimize(goal: str = "balanced", gpu_memory: float = 8.0, 
                  resolution: Tuple[int, int] = (512, 512), 
                  use_case: str = "general") -> Dict[str, Any]:
    """
    å¿«é€Ÿä¼˜åŒ–å‚æ•°
    
    Args:
        goal: ä¼˜åŒ–ç›®æ ‡ (speed, quality, balanced, memory, batch, creative)
        gpu_memory: GPUå†…å­˜å¤§å°(GB)
        resolution: ç›®æ ‡åˆ†è¾¨ç‡
        use_case: ä½¿ç”¨åœºæ™¯
        
    Returns:
        ä¼˜åŒ–åçš„å‚æ•°
    """
    # ç¡®å®šç¡¬ä»¶é…ç½®
    if gpu_memory < 4:
        hw_profile = HardwareProfile.LOW_END
    elif gpu_memory < 8:
        hw_profile = HardwareProfile.MID_RANGE
    elif gpu_memory < 12:
        hw_profile = HardwareProfile.HIGH_END
    else:
        hw_profile = HardwareProfile.ENTHUSIAST
    
    # åˆ›å»ºä¼˜åŒ–ä¸Šä¸‹æ–‡
    context = OptimizationContext(
        goal=OptimizationGoal(goal),
        hardware_profile=hw_profile,
        gpu_memory_gb=gpu_memory,
        target_resolution=resolution,
        use_case=use_case
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    optimizer = SDParameterOptimizer()
    result = optimizer.optimize_parameters(context)
    
    return result.recommended_params

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸ”§ Stable Diffusion å‚æ•°ä¼˜åŒ–å™¨")
    print("=" * 50)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = SDParameterOptimizer()
    
    # ç¤ºä¾‹1ï¼šé«˜è´¨é‡ä¼˜åŒ–
    print("\nğŸ“ˆ ç¤ºä¾‹1ï¼šé«˜è´¨é‡ä¼˜åŒ–")
    context1 = OptimizationContext(
        goal=OptimizationGoal.QUALITY,
        hardware_profile=HardwareProfile.HIGH_END,
        gpu_memory_gb=10.0,
        target_resolution=(768, 768),
        use_case="portrait"
    )
    
    result1 = optimizer.optimize_parameters(context1)
    print(f"æ¨èå‚æ•°: {result1.recommended_params['width']}x{result1.recommended_params['height']}")
    print(f"é‡‡æ ·å™¨: {result1.recommended_params['sampler_name']}")
    print(f"æ­¥æ•°: {result1.recommended_params['steps']}")
    print(f"é¢„ä¼°æ—¶é—´: {result1.estimated_time:.1f}ç§’")
    print(f"è´¨é‡åˆ†æ•°: {result1.quality_score:.2f}")
    print(f"ç½®ä¿¡åº¦: {result1.confidence_score:.2f}")
    
    # ç¤ºä¾‹2ï¼šé€Ÿåº¦ä¼˜åŒ–
    print("\nâš¡ ç¤ºä¾‹2ï¼šé€Ÿåº¦ä¼˜åŒ–")
    context2 = OptimizationContext(
        goal=OptimizationGoal.SPEED,
        hardware_profile=HardwareProfile.MID_RANGE,
        gpu_memory_gb=6.0,
        target_resolution=(512, 512),
        time_constraint=30  # 30ç§’å†…å®Œæˆ
    )
    
    result2 = optimizer.optimize_parameters(context2)
    print(f"æ¨èå‚æ•°: {result2.recommended_params['width']}x{result2.recommended_params['height']}")
    print(f"é‡‡æ ·å™¨: {result2.recommended_params['sampler_name']}")
    print(f"æ­¥æ•°: {result2.recommended_params['steps']}")
    print(f"é¢„ä¼°æ—¶é—´: {result2.estimated_time:.1f}ç§’")
    
    # ç¤ºä¾‹3ï¼šå¿«é€Ÿä¼˜åŒ–å‡½æ•°
    print("\nğŸš€ ç¤ºä¾‹3ï¼šå¿«é€Ÿä¼˜åŒ–")
    quick_params = quick_optimize(goal="balanced", gpu_memory=8.0, resolution=(768, 768))
    print(f"å¿«é€Ÿä¼˜åŒ–ç»“æœ: {quick_params['width']}x{quick_params['height']}, {quick_params['steps']}æ­¥")
    
    print("\nâœ… ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")